{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mosek\n",
    "import gc\n",
    "import cvxpy as cp\n",
    "\n",
    "#### Helper Functions\n",
    "\n",
    "def loss_gradient(y, y_hat): \n",
    "    return -(y-y_hat)\n",
    "\n",
    "def converge_test(sequence, threshold,length):\n",
    "    diff = np.diff(sequence)\n",
    "    if len(diff) < (length+1):\n",
    "        return False\n",
    "    else:\n",
    "        return (max(np.abs(diff[-length:])) < threshold)\n",
    "    \n",
    "def check_OOB_convergence(OOB_error_list):\n",
    "    if OOB_error_list[-1] < 0:\n",
    "        return True\n",
    "    elif (len(OOB_error_list) < 4):\n",
    "        return False\n",
    "    elif all([x < 10**-4 for x in OOB_error_list[-3:]]):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "### Tree Growing Functions\n",
    "\n",
    "def IncrementalDepthBaggingRegressor_fit(xTrain,yTrain,max_depth, threshold,tail):\n",
    "    train = xTrain.copy()\n",
    "    train = train.reset_index().drop('index',axis = 1)\n",
    "    train['yTrain'] = list(yTrain)\n",
    "    features = xTrain.columns\n",
    "\n",
    "    tree_list = []\n",
    "\n",
    "    for depth in range (1,max_depth+1):\n",
    "        early_stop_pred = []\n",
    "        early_stop_train_err = []\n",
    "        converged = False\n",
    "\n",
    "        while converged == False:\n",
    "\n",
    "            train1 = train.sample(n = len(train), replace = True)\n",
    "            yTrain1 = train1['yTrain']\n",
    "            xTrain1 = train1[features]\n",
    "\n",
    "            rf = DecisionTreeRegressor(max_depth = depth)\n",
    "            rf.fit(xTrain1,yTrain1)\n",
    "            tree_list.append(rf)\n",
    "            pred = rf.predict(xTrain[features])\n",
    "            \n",
    "            early_stop_pred.append(pred)\n",
    "            early_stop_train_err.append(sklearn.metrics.mean_squared_error(yTrain,(np.mean(early_stop_pred,axis = 0))))\n",
    "            #print(sklearn.metrics.mean_squared_error(yTrain,(np.mean(early_stop_pred,axis = 0))))\n",
    "            converged = converge_test(early_stop_train_err,threshold,tail)\n",
    "\n",
    "    return tree_list\n",
    "\n",
    "def IncrementalDepthBaggingRegressor_predict(xTest,tree_list):\n",
    "    pred = []\n",
    "    for clf in tree_list:\n",
    "        pred.append(clf.predict(xTest))\n",
    "    return np.mean(pred,axis = 0)\n",
    "\n",
    "\n",
    "def IncrementalDepthBagBoostRegressor_OOB_EarlyStop(xTrain,yTrain, threshold,tail):\n",
    "    train = xTrain.copy()\n",
    "    train['yTrain'] = list(yTrain)\n",
    "    features = xTrain.columns\n",
    "    pred_train = np.zeros(len(yTrain))\n",
    "    tree_list = []\n",
    "    \n",
    "    OOB_error_list = []\n",
    "    OOB_converged = False\n",
    "    depth = 1\n",
    "    while OOB_converged == False:\n",
    "    \n",
    "        early_stop_pred = []\n",
    "        early_stop_train_err = []\n",
    "        converged = False\n",
    "        OOB_matrix = []\n",
    "        tree_list1 = []\n",
    "\n",
    "        if len(tree_list) > 0:\n",
    "            current_pred = IncrementalDepthBagBoostRegressor_predict(xTrain,tree_list)\n",
    "            xTrain['current_pred'] = current_pred\n",
    "            current_pred = xTrain['current_pred']\n",
    "            xTrain.drop('current_pred',axis = 1,inplace = True)\n",
    "        else:\n",
    "            xTrain['current_pred'] = 0\n",
    "            current_pred = xTrain['current_pred']\n",
    "            xTrain.drop('current_pred',axis = 1,inplace = True)\n",
    "        \n",
    "        while converged == False:\n",
    "            \n",
    "            train1 = train.sample(n = len(train), replace = True)\n",
    "            OOB = train[~train.index.isin(train1.drop_duplicates().index.values)].index.values\n",
    "            OOB_row = np.repeat(False,len(xTrain))\n",
    "            OOB_row[OOB] = True\n",
    "            OOB_matrix.append(OOB_row)        \n",
    "            yTrain1 = train1['yTrain']\n",
    "            xTrain1 = train1[features]\n",
    "            tree = DecisionTreeRegressor(max_depth = depth)\n",
    "            tree.fit(xTrain1,yTrain1)\n",
    "            tree_list.append(tree)\n",
    "            tree_list1.append(tree)\n",
    "            pred = tree.predict(xTrain[features])\n",
    "            early_stop_pred.append(pred)\n",
    "            pred_train = pred_train + np.mean(early_stop_pred,axis = 0)\n",
    "\n",
    "            early_stop_train_err.append(sklearn.metrics.mean_squared_error(yTrain,pred_train))\n",
    "            converged = converge_test(early_stop_train_err,threshold,tail)\n",
    "\n",
    "            if converged == False:\n",
    "                pred_train = pred_train - np.mean(early_stop_pred,axis = 0)\n",
    "                   \n",
    "        ### compute OOB\n",
    "        indicators = pd.DataFrame(OOB_matrix).transpose()\n",
    "        OOB_pred_list = []\n",
    "        yTrain2 = yTrain.copy()\n",
    "        \n",
    "        for i,row in xTrain.iterrows():\n",
    "            row = row.to_frame().transpose()\n",
    "            temp_series = indicators.iloc[i]\n",
    "            OOB_trees = list(temp_series[temp_series].index.values)\n",
    "            OOB_tree_list = list(np.array(tree_list1)[OOB_trees])\n",
    "            \n",
    "        \n",
    "            if len(OOB_tree_list) > 0:\n",
    "                OOB_pred = []\n",
    "                for tree_temp in OOB_tree_list:\n",
    "                    OOB_pred.append(tree_temp.predict(row)[0])\n",
    "                OOB_pred_list.append(np.mean(OOB_pred))\n",
    "            else:\n",
    "                yTrain2 = yTrain2.drop(i)\n",
    "                current_pred = current_pred.drop(i)\n",
    "        \n",
    "        next_pred = np.array(current_pred) + np.array(OOB_pred_list)\n",
    "        current_err = sklearn.metrics.mean_squared_error(yTrain2,current_pred)\n",
    "        next_err = sklearn.metrics.mean_squared_error(yTrain2,next_pred)\n",
    "        print(current_err,next_err, depth)\n",
    "        OOB_error_list.append(current_err-next_err)\n",
    "        \n",
    "        residuals = -loss_gradient(yTrain, pred_train) \n",
    "        train['yTrain'] = residuals.values\n",
    "    \n",
    "        OOB_converged = check_OOB_convergence(OOB_error_list)\n",
    "        depth = depth + 1\n",
    "    \n",
    "    return tree_list\n",
    "\n",
    "def IncrementalDepthBagBoostRegressor_predict(xTest,tree_list):\n",
    "    res = []\n",
    "    for i in tree_list:\n",
    "        depth = i.max_depth\n",
    "        pred = i.predict(xTest)\n",
    "        res.append([depth,pred])\n",
    "    res = pd.DataFrame(res,columns = ['depth','pred'])\n",
    "    res = res.groupby('depth')['pred'].apply(np.mean).reset_index()\n",
    "    res = np.sum(res['pred'].to_numpy())\n",
    "    return res\n",
    "\n",
    "### LASSO Functions\n",
    "\n",
    "def OptimizationStepRegression(xTrain,yTrain,tree_list,lambd, optimization_type = 'penalized'):\n",
    "    pred = []\n",
    "    ind = []\n",
    "    for tree in tree_list:\n",
    "        pred.append(tree.predict(xTrain))\n",
    "        ind.append([int(x > 0) for x in tree.feature_importances_])  \n",
    "\n",
    "    pred = np.transpose(pred)\n",
    "    ind = np.transpose(ind)\n",
    "    \n",
    "    w = cp.Variable(len(tree_list),nonneg=True)\n",
    "    constraints = []\n",
    "    if optimization_type == 'penalized':\n",
    "        loss = cp.sum_squares(cp.matmul(pred,w)-yTrain) \n",
    "        objective = (1/len(yTrain))*loss + lambd*cp.norm(cp.matmul(ind,w),1)\n",
    "    if optimization_type == 'constrained':\n",
    "        objective = cp.sum_squares(cp.matmul(pred,w)-yTrain) \n",
    "        constraints = [cp.norm(cp.matmul(ind,w),1)<= lambd]\n",
    "\n",
    "    prob = cp.Problem(cp.Minimize(objective),constraints)\n",
    "    prob.solve(solver = cp.MOSEK,mosek_params = {mosek.dparam.optimizer_max_time: 10000.0} )\n",
    "    weights = np.asarray(w.value)\n",
    "    weights[np.abs(weights) < 10**-3] = 0 \n",
    "    return weights\n",
    "\n",
    "def ControlBurnRegressor_select_features(xTest, tree_list,weights):\n",
    "    imp = []\n",
    "    for i in range(0,len(weights)):\n",
    "        imp.append(weights[i]*tree_list[i].feature_importances_)\n",
    "    imp1 = np.sum(imp, axis = 0)\n",
    "    return imp1\n",
    "\n",
    "def ControlBurnRegressor_predict(xTest, tree_list, weights):\n",
    "    res = []\n",
    "    for i in range(0,len(tree_list)):\n",
    "        res.append(weights[i]*tree_list[i].predict(xTest))\n",
    "    return np.sum(res,axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def load_openml(dataset,y_label = ''):\n",
    "    dataset1 = sklearn.datasets.fetch_openml(dataset,as_frame = True)\n",
    "    X, y = dataset1.data, dataset1.target \n",
    "    data = pd.DataFrame(X,columns = dataset1.feature_names)\n",
    "    \n",
    "    if len(y_label) == 0:\n",
    "        data['y'] = y\n",
    "    else:\n",
    "        data['y'] = y[y_label]\n",
    "        \n",
    "    data = data.sample(frac = 1)\n",
    "    y = data['y']\n",
    "    X = data.drop('y',axis = 1)\n",
    "    cat = list(set(X.columns) - set(X.select_dtypes(include=np.number).columns.tolist()))\n",
    "    for col in cat:\n",
    "        X[col] = X[col].astype('category').cat.codes\n",
    "        X[col] = X[col].fillna(max(X[col]+1))\n",
    "    X = X.fillna(X.median()) \n",
    "    return X,y\n",
    "\n",
    "def baseline_regressor(xTrain,yTrain,xTest,yTest, num_features, num_trees):\n",
    "    model = RandomForestRegressor(n_estimators = num_trees)\n",
    "    rf = model.fit(xTrain,yTrain)\n",
    "    imp = pd.DataFrame(np.column_stack((xTrain.columns,rf.feature_importances_)),columns = ['features','scores'])\n",
    "    imp = imp.sort_values('scores',ascending = False)\n",
    "    to_use = imp.head(num_features)['features'].values\n",
    "    rf1 = model.fit(xTrain[to_use],yTrain)\n",
    "    pred = rf1.predict(xTest[to_use])\n",
    "    return sklearn.metrics.mean_squared_error(yTest,pred)\n",
    "\n",
    "def evaluate_regression_experiment(xTrain,yTrain,xTest,yTest,lambd,tree_list,form):\n",
    "    weights = OptimizationStepRegression(xTrain, yTrain,tree_list, lambd,form)\n",
    "    pred_no_polish = ControlBurnRegressor_predict(xTest, tree_list,weights)\n",
    "    acc_no_polish = sklearn.metrics.mean_squared_error(yTest,pred_no_polish)\n",
    "\n",
    "    imp1 = ControlBurnRegressor_select_features(xTest, tree_list,weights)\n",
    "    \n",
    "    if sum(imp1>0) == 0:\n",
    "        return [1,1,1,0]\n",
    "    \n",
    "    rf = RandomForestRegressor(n_estimators = 100).fit(xTrain[xTrain.columns[imp1 > 0]],yTrain)\n",
    "    pred_polish = rf.predict(xTest[xTrain.columns[imp1 > 0]])\n",
    "    acc_polish = sklearn.metrics.mean_squared_error(yTest,pred_polish)\n",
    "\n",
    "    importances = pd.DataFrame(np.column_stack((xTrain.columns,imp1)),columns = ['features','scores'])\n",
    "    importances = importances.sort_values('scores',ascending = False)\n",
    "    num_features = np.sum(importances['scores'] != 0)\n",
    "    \n",
    "    acc_base = baseline_regressor(xTrain,yTrain,xTest,yTest, np.sum(imp1>0), len(tree_list))\n",
    "\n",
    "    return [acc_no_polish,acc_polish,acc_base,num_features]\n",
    "\n",
    "\n",
    "def bisection_lambd(xTrain,yTrain,xTest,yTest,tree_list,form,lambd,count_limit,feature_to_find):\n",
    "    counter = 0\n",
    "    to_find = 0\n",
    "    total_results = []\n",
    "    while to_find <= feature_to_find:\n",
    "        results = evaluate_regression_experiment(xTrain,yTrain,xTest,yTest,lambd,tree_list,form)\n",
    "        nfeat = results[3]\n",
    "        print(to_find,nfeat,lambd)\n",
    "\n",
    "        if nfeat == to_find:\n",
    "            to_find = to_find + 1\n",
    "\n",
    "        elif counter > count_limit:\n",
    "            to_find = to_find + 1\n",
    "            counter = 0\n",
    "\n",
    "        elif nfeat < to_find:\n",
    "            lambd = lambd/2\n",
    "\n",
    "        elif nfeat > to_find:\n",
    "            lambd = lambd + lambd/2\n",
    "\n",
    "        counter = counter + 1\n",
    "\n",
    "        total_results.append(results)\n",
    "    return total_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ControlBurnRegressorExperiment(dataset,y_label,form,lambd_start,count_limit,features_to_find):\n",
    "    X,y = load_openml(dataset,y_label = y_label)\n",
    "    kf = KFold(n_splits=5)\n",
    "    kf.get_n_splits(X)\n",
    "    final_result_bagboost = pd.DataFrame(None)\n",
    "    final_result_bag = pd.DataFrame(None)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        xTrain, xTest = X.iloc[train_index], X.iloc[test_index]\n",
    "        yTrain, yTest = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        features = xTrain.columns\n",
    "        xTrain = preprocessing.scale(xTrain)\n",
    "        xTrain = pd.DataFrame(xTrain,columns = features)\n",
    "        xTest = preprocessing.scale(xTest)\n",
    "        xTest = pd.DataFrame(xTest,columns = features)\n",
    "        yTest = pd.Series(preprocessing.scale(yTest))\n",
    "        yTrain = pd.Series(preprocessing.scale(yTrain))\n",
    "\n",
    "\n",
    "        tree_list = IncrementalDepthBagBoostRegressor_OOB_EarlyStop(xTrain,yTrain,10**-3,5)\n",
    "        res = bisection_lambd(xTrain,yTrain,xTest,yTest,tree_list,form,lambd_start,count_limit,features_to_find)\n",
    "        res = pd.DataFrame(res, columns = ['no_polish','polish','base','nonzero'])\n",
    "        final_result_bagboost = final_result_bagboost.append(res)\n",
    "        \n",
    "        \n",
    "        tree_list_bag = IncrementalDepthBaggingRegressor_fit(xTrain,yTrain,25,10**-3,5)\n",
    "        res_bag = bisection_lambd(xTrain,yTrain,xTest,yTest,tree_list,form,lambd_start,count_limit,features_to_find)\n",
    "        res_bag = pd.DataFrame(res_bag, columns = ['no_polish','polish','base','nonzero'])\n",
    "        final_result_bag = final_result_bag.append(res_bag)\n",
    "\n",
    "    return final_result_bagboost, final_results_bag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "dataset = 'boston'\n",
    "X,y = load_openml(dataset,y_label = '')\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(X)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    xTrain, xTest = X.iloc[train_index], X.iloc[test_index]\n",
    "    yTrain, yTest = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "dataset = 'boston'\n",
    "X,y = load_openml(dataset,y_label = '')\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "final_result_bagboost = pd.DataFrame(None)\n",
    "final_result_bag = pd.DataFrame(None)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    xTrain, xTest = X.iloc[train_index], X.iloc[test_index]\n",
    "    yTrain, yTest = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    features = xTrain.columns\n",
    "    xTrain = preprocessing.scale(xTrain)\n",
    "    xTrain = pd.DataFrame(xTrain,columns = features)\n",
    "    xTest = preprocessing.scale(xTest)\n",
    "    xTest = pd.DataFrame(xTest,columns = features)\n",
    "    yTest = pd.Series(preprocessing.scale(yTest))\n",
    "    yTrain = pd.Series(preprocessing.scale(yTrain))\n",
    "    \n",
    "\n",
    "    tree_list = IncrementalDepthBagBoostRegressor_OOB_EarlyStop(xTrain,yTrain,10**-3,5)\n",
    "    res = bisection_lambd(xTrain,yTrain,xTest,yTest,tree_list,'penalized',10,8,10)\n",
    "    res = pd.DataFrame(res, columns = ['no_polish','polish','base','nonzero'])\n",
    "    final_result_bagboost = final_result_bagboost.append(res)\n",
    "    \n",
    "    tree_list_bag = IncrementalDepthBaggingRegressor_fit(xTrain,yTrain,25,10**-3,5)\n",
    "    res_bag = bisection_lambd(xTrain,yTrain,xTest,yTest,tree_list_bag,'penalized',10,8,10)\n",
    "    res_bag = pd.DataFrame(res_bag, columns = ['no_polish','polish','base','nonzero'])\n",
    "    final_result_bag = final_result_bag.append(res_bag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_agg = final_result_bagboost.groupby('nonzero').agg(['mean','std']).reset_index()\n",
    "final_result_agg = final_result_agg[final_result_agg['nonzero'] != 0]\n",
    "final_result_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_bag_agg = final_result_bag.groupby('nonzero').agg(['mean','std']).reset_index()\n",
    "final_result_bag_agg = final_result_bag_agg[final_result_bag_agg['nonzero'] != 0]\n",
    "final_result_bag_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(final_result_agg['nonzero'],final_result_agg['base']['mean'],label = 'baseline', color = 'blue')\n",
    "plt.scatter(final_result_agg['nonzero'],final_result_agg['base']['mean'],color = 'blue')\n",
    "plt.errorbar(final_result_agg['nonzero'],final_result_agg['base']['mean'],yerr = final_result_agg['base']['std'],color = 'blue')\n",
    "\n",
    "\n",
    "plt.plot(final_result_agg['nonzero'],final_result_agg['polish']['mean'],label = 'polished',color = 'green')\n",
    "plt.scatter(final_result_agg['nonzero'],final_result_agg['polish']['mean'], color = 'green')\n",
    "plt.errorbar(final_result_agg['nonzero'],final_result_agg['polish']['mean'],yerr = final_result_agg['polish']['std'], color = 'green')\n",
    "\n",
    "#plt.plot(final_result_agg['nonzero'],final_result_agg['no_polish']['mean'],label = 'nonpolished', color = 'orange')\n",
    "#plt.scatter(final_result_agg['nonzero'],final_result_agg['no_polish']['mean'], color = 'orange')\n",
    "#plt.errorbar(final_result_agg['nonzero'],final_result_agg['no_polish']['mean'],yerr = final_result_agg['no_polish']['std'], color = 'orange')\n",
    "#plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(final_result_bag_agg['nonzero'],final_result_bag_agg['base']['mean'],label = 'baseline', color = 'blue')\n",
    "plt.scatter(final_result_bag_agg['nonzero'],final_result_bag_agg['base']['mean'],color = 'blue')\n",
    "plt.errorbar(final_result_bag_agg['nonzero'],final_result_bag_agg['base']['mean'],yerr = final_result_bag_agg['base']['std'],color = 'blue')\n",
    "\n",
    "\n",
    "plt.plot(final_result_bag_agg['nonzero'],final_result_bag_agg['polish']['mean'],label = 'polished',color = 'green')\n",
    "plt.scatter(final_result_bag_agg['nonzero'],final_result_bag_agg['polish']['mean'], color = 'green')\n",
    "plt.errorbar(final_result_bag_agg['nonzero'],final_result_bag_agg['polish']['mean'],yerr = final_result_bag_agg['polish']['std'], color = 'green')\n",
    "\n",
    "#plt.plot(final_result_agg['nonzero'],final_result_agg['no_polish']['mean'],label = 'nonpolished', color = 'orange')\n",
    "#plt.scatter(final_result_agg['nonzero'],final_result_agg['no_polish']['mean'], color = 'orange')\n",
    "#plt.errorbar(final_result_agg['nonzero'],final_result_agg['no_polish']['mean'],yerr = final_result_agg['no_polish']['std'], color = 'orange')\n",
    "#plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "dataset = 'topo_2_1'\n",
    "X,y = load_openml(dataset,y_label = '')\n",
    "print(len(X),len(X.columns))\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "final_result_bagboost = pd.DataFrame(None)\n",
    "final_result_bag = pd.DataFrame(None)\n",
    "folds_data = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    xTrain, xTest = X.iloc[train_index], X.iloc[test_index]\n",
    "    yTrain, yTest = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    features = xTrain.columns\n",
    "    xTrain = preprocessing.scale(xTrain)\n",
    "    xTrain = pd.DataFrame(xTrain,columns = features)\n",
    "    xTest = preprocessing.scale(xTest)\n",
    "    xTest = pd.DataFrame(xTest,columns = features)\n",
    "    yTest = pd.Series(preprocessing.scale(yTest))\n",
    "    yTrain = pd.Series(preprocessing.scale(yTrain))\n",
    "    \n",
    "    folds_data.append([xTrain,xTest,yTrain,yTest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import ray\n",
    "import time\n",
    "\n",
    "# Start Ray.\n",
    "ray.init()\n",
    "\n",
    "@ray.remote\n",
    "def parallel_wrapper(arg):\n",
    "    xTrain = arg[0]\n",
    "    xTest = arg[1]\n",
    "    yTrain = arg[2]\n",
    "    yTest = arg[3]\n",
    "    \n",
    "    features_to_find = min(len(X.columns),10)\n",
    "    \n",
    "    tree_list = IncrementalDepthBagBoostRegressor_OOB_EarlyStop(xTrain,yTrain,10**-3,5)\n",
    "    res = bisection_lambd(xTrain,yTrain,xTest,yTest,tree_list,'penalized',10,8,features_to_find)\n",
    "    res = pd.DataFrame(res, columns = ['no_polish','polish','base','nonzero'])\n",
    "\n",
    "    tree_list_bag = IncrementalDepthBaggingRegressor_fit(xTrain,yTrain,25,10**-3,5)\n",
    "    res_bag = bisection_lambd(xTrain,yTrain,xTest,yTest,tree_list_bag,'penalized',10,8,features_to_find)\n",
    "    res_bag = pd.DataFrame(res_bag, columns = ['no_polish','polish','base','nonzero'])\n",
    "    \n",
    "    return res,res_bag\n",
    "\n",
    "result_ids = []\n",
    "for i in folds_data:\n",
    "    result_ids.append(parallel_wrapper.remote(i))\n",
    "    \n",
    "\n",
    "results = ray.get(result_ids)  \n",
    "ray.shutdown()\n",
    "\n",
    "final_result_bagboost = pd.DataFrame(None)\n",
    "final_result_bag = pd.DataFrame(None)\n",
    "for r in results:\n",
    "    final_result_bagboost = final_result_bagboost.append(r[0])\n",
    "    final_result_bag = final_result_bag.append(r[1])\n",
    "    \n",
    "final_result_agg = final_result_bagboost.groupby('nonzero').agg(['mean','std']).reset_index()\n",
    "final_result_agg = final_result_agg[final_result_agg['nonzero'] != 0]\n",
    "final_result_bag_agg = final_result_bag.groupby('nonzero').agg(['mean','std']).reset_index()\n",
    "final_result_bag_agg = final_result_bag_agg[final_result_bag_agg['nonzero'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(final_result_agg['nonzero'],final_result_agg['base']['mean'],label = 'baseline', color = 'blue')\n",
    "plt.scatter(final_result_agg['nonzero'],final_result_agg['base']['mean'],color = 'blue')\n",
    "plt.errorbar(final_result_agg['nonzero'],final_result_agg['base']['mean'],yerr = final_result_agg['base']['std'],color = 'blue')\n",
    "\n",
    "\n",
    "plt.plot(final_result_agg['nonzero'],final_result_agg['polish']['mean'],label = 'polished',color = 'green')\n",
    "plt.scatter(final_result_agg['nonzero'],final_result_agg['polish']['mean'], color = 'green')\n",
    "plt.errorbar(final_result_agg['nonzero'],final_result_agg['polish']['mean'],yerr = final_result_agg['polish']['std'], color = 'green')\n",
    "\n",
    "plt.plot(final_result_agg['nonzero'],final_result_agg['no_polish']['mean'],label = 'nonpolished', color = 'orange')\n",
    "plt.scatter(final_result_agg['nonzero'],final_result_agg['no_polish']['mean'], color = 'orange')\n",
    "plt.errorbar(final_result_agg['nonzero'],final_result_agg['no_polish']['mean'],yerr = final_result_agg['no_polish']['std'], color = 'orange')\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(dataset+ ' bagboost')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Features Selected')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(final_result_bag_agg['nonzero'],final_result_bag_agg['base']['mean'],label = 'baseline', color = 'navy')\n",
    "plt.scatter(final_result_bag_agg['nonzero'],final_result_bag_agg['base']['mean'],color = 'navy')\n",
    "plt.errorbar(final_result_bag_agg['nonzero'],final_result_bag_agg['base']['mean'],yerr = final_result_bag_agg['base']['std'],color = 'navy')\n",
    "\n",
    "\n",
    "plt.plot(final_result_bag_agg['nonzero'],final_result_bag_agg['polish']['mean'],label = 'polished',color = 'olive')\n",
    "plt.scatter(final_result_bag_agg['nonzero'],final_result_bag_agg['polish']['mean'], color = 'olive')\n",
    "plt.errorbar(final_result_bag_agg['nonzero'],final_result_bag_agg['polish']['mean'],yerr = final_result_bag_agg['polish']['std'], color = 'olive')\n",
    "\n",
    "plt.plot(final_result_bag_agg['nonzero'],final_result_bag_agg['no_polish']['mean'],label = 'nonpolished', color = 'salmon')\n",
    "plt.scatter(final_result_bag_agg['nonzero'],final_result_bag_agg['no_polish']['mean'], color = 'salmon')\n",
    "plt.errorbar(final_result_bag_agg['nonzero'],final_result_bag_agg['no_polish']['mean'],yerr = final_result_bag_agg['no_polish']['std'], color = 'salmon')\n",
    "\n",
    "plt.legend()\n",
    "plt.title(dataset + ' bag')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Features Selected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_agg.to_csv('..Results/'+dataset+'_bagboost.csv')\n",
    "final_result_bag_agg.to_csv('..Results/'+dataset+'_bag.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
